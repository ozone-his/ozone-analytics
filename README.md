# Mekom data pipelines
This project hosts Mekom's Flink powered data pipelines. It is composed of two Flink Jobs one for handling the streaming data pipelines which uses kafka and kafka connect to streaming OpenMRS data in a MySQL database into flatten tables in PostgreSQL
The second Flink job is used for exporting flatted data generated by the streaming job  into parquet files for purpose of moving the data into a centralised warehose.

The project also includes Minio for centralised parquet files storage with drill used to query the data directly for Minio.

The services have been split into multiple files this allow you to start only the services you for example you could only be interested in start the streaming data pipelines and not superset. All you may want to run  minio/drill on separate server and just upload  parquet exported for remote data pipelines.

## Architecture

### Streaming analytics pipelines

![Streaming](readme/Streaming.jpg)

### Centralized analytics
![Centralized](readme/Centralized.jpg)


## Use Cases

### Streaming Data Pipelines Only
In cases where you don't need to start Superset for example when you will use the  Parquet export  job to create parquet files which you load into Minio or S3 and query with Drill on a separate server, you can start only the streaming data pipelines by running.

`$ docker compose -f docker-compose-data-pipelines.yaml up -d --build`

Which will start ;

* [ZooKeeper](https://zookeeper.apache.org/ "ZooKeeper") - Used by Flink for High Availability ensuring we can always recover when a job fails or is stopped. WIthout this Flink job will restart the streaming every time it is stopped and restarted
* [Kafka Connect](https://docs.confluent.io/platform/current/connect/ "Kafka Connect")  - (Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems) In the contect of this project Kafka Connect is used as means of deploying [Debezium](https://debezium.io/documentation/reference/stable/architecture.html "Debezium")
* [Kafka](https://kafka.apache.org/ "Kafka") - Used for storing streaming events from MySQL 
* [Kowl](https://github.com/redpanda-data/kowl "Kowl") - Provides and UI for managing Kafka
* [Flink Job manager](https://nightlies.apache.org/flink/flink-docs-master/docs/internals/job_scheduling/ "Flink Jobmanager") - Coordinates the Flink cluster 
* [Flink Task manager](https://nightlies.apache.org/flink/flink-docs-master/docs/internals/task_lifecycle/ "Flink Taskmanager") - Runs the actual Workloads assigned by the Job manager
* [MySQL](https://www.mysql.com/ "MySQL") - This contains a data dump from OpenMRS used for demo purposes with this project
* [PostgresSQL](https://www.postgresql.org/ "PostgresSQL") - This is the sink database where the streaming pipelines output the flattened data. Once the data is flatted it can be used directly for analytics by Superset or Exported to Parquet for external storage and  using other anaytics  tools.

###  Streaming Data Pipelines with Superset for Visualisation
If you want to start Superset BI  alongside the streaming data pipeline run

`$ docker compose -f docker-compose-data-pipelines.yaml -f docker-compose-superset.yaml up -d --build`
This will start an extra two services

* [Redis](https://redis.io/ "Redis") - Used as a backgroud task queue for Superset
* [Superset](https://superset.apache.org/ "Superset") - This is an OpenSource BI tool originaly created at AirBnB 
* [Superset Worker](https://superset.apache.org/docs/intro "Superset Worker") - Run Superset background tasks


> NOTE: The streaming jobs may fail for a while during the initial start up as Flink discovers data partitions from Kafka you can wait for this to sort itself out or you can try to restart the jobmanager and taskmanager with `docker compose -f docker-compose-data-pipelines.yaml restart jobmanager taskmanager`

### Demo data

The demo data included with this project is a dump from OpenMRS reference application configured to generate 155 patients and is loaded into mysql during initialization

### Drill Analytics Server (Remote parquet files)

In cases where you have multiple instances of OpenMRS or Ozone deployed in remote locations you may what to process data onsite with streaming pipelines but run ship the data to a central repostory for analytics. This provides a sultions that uses
* [Minio](https://min.io/ "Minio") - An S3 compatible object storage server
* [Drill](https://drill.apache.org/ "Drill") - A Schema-free SQL Query Engine for Hadoop, NoSQL and Cloud Storage and
* Superset -  An OpenSource BI tool originaly created at AirBnB 
to provide a centralized analyics platform.

To start this stack run;

`$ docker compose -f docker-compose-superset.yaml -f docker-compose-mino.yaml -f docker-compose-drill.yaml up -d --build`

### Parquet Export Job
In the case where you are running remote streaming pipelines to flatten data on site but want to ship it to a the central repository described above you can run th export job to generate data inside of the `./parquet`  folder of this project whit you will upload  to the `openmrs-data` bucket on the Minio server

`$ docker compose -f docker-compose-data-pipelines.yaml -f docker-compose-export.yaml up parquet-export  --build`

### Service coordinates
| Service  |   Access| Credentials|
| ------------ | ------------ |------------ |
| Kowl  |  http://localhost:8282 | |
| Flink  |  http://localhost:8084 | |
| Superset  | http://localhost:8088  | admin/password|
| Minio   | http://localhost:9000   |minioadmin/minioadmin123|
| Drill  |  http://localhost:8047 | |
